{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2bdeac1a",
   "metadata": {},
   "source": [
    "## Hybrid Retrieval Engine - PoC\n",
    "Here, we will combine the 2 layered hybrid retrieval engine - \n",
    "1. Stage 1 - Lexical: Sparse Retrieval using BM25\n",
    "2. Stage 2 - Semantic: Dense Retrieval using Vector Embeddings (Gemini: `gemini-embedding-001` embedding model)\n",
    "\n",
    "Let's get started."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "206fd201",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "root = Path().absolute().parents[1]\n",
    "os.chdir(str(root))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dfac114",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "438dd3a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import bm25s\n",
    "import Stemmer\n",
    "from google.genai import types\n",
    "from langchain_core.documents import Document\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from langchain_community.docstore.in_memory import InMemoryDocstore\n",
    "\n",
    "\n",
    "class HybridRetrieval:\n",
    "    def __init__(self, top_k_stage_1: int, top_k_stage_2: int):\n",
    "        self.top_k_stage_1 = top_k_stage_1\n",
    "        self.top_k_stage_2 = top_k_stage_2\n",
    "\n",
    "    def _instantiate_stage_1(self):\n",
    "        self.stemmer = Stemmer.Stemmer(\"english\")\n",
    "        self.retriever = bm25s.BM25.load(\"artifacts/bm25\", load_corpus=True)\n",
    "        with open(\"artifacts/bm25/corpus.jsonl\", \"r\") as f:\n",
    "            self.corpus = [json.loads(line) for line in f]\n",
    "\n",
    "    def _instantiate_stage_2(self):\n",
    "        embeddings = GoogleGenerativeAIEmbeddings(\n",
    "            model=\"gemini-embedding-001\",\n",
    "            task_type=\"semantic_similarity\",\n",
    "            config=types.EmbedContentConfig(\n",
    "                    output_dimensionality=3072,\n",
    "                    task_type=\"SEMANTIC_SIMILARITY\",\n",
    "                )\n",
    "        )\n",
    "        self.vector_store = FAISS.load_local(\n",
    "            folder_path=\"artifacts/cso_smry/faiss_index\",\n",
    "            embeddings=embeddings,\n",
    "            allow_dangerous_deserialization=True\n",
    "        )\n",
    "\n",
    "    def search(self, query: str):\n",
    "        # Stage 1: Lexical Search\n",
    "        query_tokens = bm25s.tokenize(query, stemmer=self.stemmer)\n",
    "        docs, scores = self.retriever.retrieve(query_tokens, k=self.top_k_stage_1, corpus=self.corpus)\n",
    "        stage_1_results = {\n",
    "            doc[\"id\"] : float(score) for doc, score in zip(docs[0], scores[0])\n",
    "        }\n",
    "\n",
    "        # Stage 2: Semantic Search\n",
    "        stage_2_results = self.vector_store.similarity_search(query, top_k=self.top_k_stage_2)\n",
    "\n",
    "        # Combine results from both stages\n",
    "        combined_results = stage_1_results + stage_2_results\n",
    "        return combined_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a49a40",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
